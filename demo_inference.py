"""
Script de demostraci√≥n para probar el an√°lisis de pronunciaci√≥n
con el modelo Whisper fine-tuneado usando muestras reales del dataset.
"""

import os
import sys
import logging
import argparse
from typing import List, Dict
import json

import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import librosa

from pronunciation_analyzer import PronunciationAnalyzer, PronunciationAnalysisResult
from data_loader import SpeechOceanDataLoader

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Importaci√≥n opcional de IPython para notebooks
try:
    from IPython.display import Audio, display
    IPYTHON_AVAILABLE = True
except ImportError:
    IPYTHON_AVAILABLE = False


class PronunciationDemo:
    """
    Clase de demostraci√≥n para mostrar las capacidades del analizador
    de pronunciaci√≥n con ejemplos reales del dataset speechocean762.
    """
    
    def __init__(self, model_path: str, base_model: str = "openai/whisper-small"):
        self.model_path = model_path
        self.base_model = base_model
        
        # Verificar que el modelo existe
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Modelo no encontrado en: {model_path}")
        
        # Inicializar analizador
        logger.info("Cargando analizador de pronunciaci√≥n...")
        self.analyzer = PronunciationAnalyzer(
            model_path=model_path,
            base_model=base_model
        )
        logger.info("‚úÖ Analizador cargado exitosamente")
        
        # Inicializar data loader para acceder al dataset
        logger.info("Cargando dataset speechocean762...")
        self.data_loader = SpeechOceanDataLoader(
            model_name=base_model,
            cache_dir="./cache"
        )
        self.dataset = None
    
    def load_real_samples(self, num_samples: int = 3) -> List[Dict]:
        """Cargar muestras reales del dataset speechocean762"""
        try:
            if self.dataset is None:
                self.dataset = self.data_loader.load_dataset()
            
            # Usar muestras del conjunto de test
            test_dataset = self.dataset["test"]
            
            # Seleccionar muestras variadas (diferentes scores)
            samples = []
            indices = [0, len(test_dataset)//4, len(test_dataset)//2]  # Muestras distribuidas
            
            for i, idx in enumerate(indices[:num_samples]):
                if idx < len(test_dataset):
                    sample = test_dataset[idx]
                    
                    # Procesar audio
                    try:
                        audio_array = self.data_loader.preprocess_audio(sample["audio"])
                        
                        sample_info = {
                            "id": idx,
                            "text": sample["text"],
                            "audio_array": audio_array,
                            "real_scores": {
                                "accuracy": sample.get("accuracy", 0.0),
                                "fluency": sample.get("fluency", 0.0),
                                "completeness": sample.get("completeness", 0.0),
                                "prosodic": sample.get("prosodic", 0.0),
                                "total": sample.get("total", 0.0)
                            },
                            "words": sample.get("words", []),
                            "description": f"Muestra real #{idx+1} del dataset"
                        }
                        samples.append(sample_info)
                        logger.info(f"‚úÖ Cargada muestra {i+1}: '{sample['text'][:50]}...'")
                        
                    except Exception as e:
                        logger.warning(f"Error procesando muestra {idx}: {e}")
                        continue
            
            if not samples:
                logger.warning("No se pudieron cargar muestras del dataset, usando audio sint√©tico")
                return self.get_fallback_samples()
                
            return samples
            
        except Exception as e:
            logger.error(f"Error cargando dataset: {e}")
            logger.info("Usando muestras sint√©ticas como fallback...")
            return self.get_fallback_samples()
    
    def get_fallback_samples(self) -> List[Dict]:
        """Muestras sint√©ticas como fallback si no se puede cargar el dataset"""
        logger.info("Generando muestras sint√©ticas...")
        
        sample_texts = [
            "Hello world",
            "The quick brown fox jumps over the lazy dog",  
            "Pronunciation is very important for communication"
        ]
        
        samples = []
        for i, text in enumerate(sample_texts):
            # Generar audio sint√©tico mejorado
            audio_array = self.generate_speech_like_audio(text)
            
            sample_info = {
                "id": f"synthetic_{i}",
                "text": text,
                "audio_array": audio_array,
                "real_scores": None,  # No hay scores reales para sint√©tico
                "words": [],
                "description": f"Muestra sint√©tica #{i+1}"
            }
            samples.append(sample_info)
        
        return samples
    
    def generate_speech_like_audio(self, text: str, duration: float = None) -> np.ndarray:
        """Generar audio sint√©tico mejorado (solo como fallback)"""
        if duration is None:
            duration = max(1.0, len(text) * 0.08)  # ~0.08s por car√°cter
        
        sample_rate = 16000
        t = np.linspace(0, duration, int(sample_rate * duration))
        
        # Crear audio m√°s realista con formantes
        fundamental_freq = 120 + np.random.uniform(-20, 20)
        audio = np.zeros_like(t)
        
        # Simular palabras
        words = text.split()
        word_duration = duration / len(words)
        
        for i, word in enumerate(words):
            start_time = i * word_duration
            end_time = (i + 1) * word_duration
            
            start_idx = int(start_time * sample_rate)
            end_idx = int(end_time * sample_rate)
            
            if start_idx < len(audio) and end_idx <= len(audio):
                word_t = t[start_idx:end_idx]
                
                # Formantes variando por palabra
                f1 = 500 + np.random.uniform(-100, 200)
                f2 = 1500 + np.random.uniform(-300, 500)
                
                word_audio = np.zeros_like(word_t)
                
                # M√∫ltiples arm√≥nicos
                for harmonic in range(1, 5):
                    freq = fundamental_freq * harmonic
                    if freq < sample_rate // 2:
                        amplitude = 1.0 / harmonic
                        word_audio += amplitude * np.sin(2 * np.pi * freq * word_t)
                
                # Agregar formantes
                word_audio += 0.3 * np.sin(2 * np.pi * f1 * word_t)
                word_audio += 0.2 * np.sin(2 * np.pi * f2 * word_t)
                
                # Envolvente natural
                envelope = np.exp(-2 * word_t / word_duration)
                word_audio *= envelope
                
                audio[start_idx:end_idx] = word_audio
        
        # Ruido y filtrado
        noise = np.random.normal(0, 0.05, len(audio))
        audio = audio + noise
        
        # Normalizar
        max_val = np.max(np.abs(audio))
        if max_val > 0:
            audio = audio / max_val * 0.7
        
        return audio
    
    def analyze_real_samples(self) -> List[Dict]:
        """Analizar muestras reales del dataset"""
        
        # Cargar muestras reales
        samples = self.load_real_samples()
        
        results = []
        
        for i, sample in enumerate(samples):
            logger.info(f"Analizando muestra {i+1}: '{sample['text'][:50]}{'...' if len(sample['text']) > 50 else ''}'")
            
            try:
                # Analizar pronunciaci√≥n con el modelo
                analysis = self.analyzer.analyze_pronunciation(
                    audio_array=sample["audio_array"],
                    sampling_rate=16000,
                    expected_text=sample["text"]
                )
                
                result = {
                    "sample_info": sample,
                    "analysis": analysis,
                    "real_scores": sample["real_scores"],  # Scores reales del dataset
                    "audio": sample["audio_array"]
                }
                
                results.append(result)
                
            except Exception as e:
                logger.error(f"Error analizando muestra {i+1}: {e}")
        
        return results
    
    def create_visualization(self, results: List[Dict]) -> None:
        """Crear visualizaciones de los resultados"""
        
        if not results:
            logger.warning("No hay resultados para visualizar")
            return
        
        # Configurar estilo
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('An√°lisis de Pronunciaci√≥n - Resultados', fontsize=16, fontweight='bold')
        
        # Extraer datos para visualizaci√≥n
        sample_names = [f"Muestra {i+1}" for i in range(len(results))]
        accuracy_scores = []
        fluency_scores = []
        completeness_scores = []
        prosodic_scores = []
        
        for result in results:
            analysis = result["analysis"]
            accuracy_scores.append(analysis.scores.accuracy)
            fluency_scores.append(analysis.scores.fluency)
            completeness_scores.append(analysis.scores.completeness * 10)  # Escalar a 0-10
            prosodic_scores.append(analysis.scores.prosodic)
        
        # Gr√°fico 1: Puntuaciones por categor√≠a
        ax1 = axes[0, 0]
        x = np.arange(len(sample_names))
        width = 0.2
        
        ax1.bar(x - 1.5*width, accuracy_scores, width, label='Precisi√≥n', alpha=0.8)
        ax1.bar(x - 0.5*width, fluency_scores, width, label='Fluidez', alpha=0.8)
        ax1.bar(x + 0.5*width, completeness_scores, width, label='Completitud', alpha=0.8)
        ax1.bar(x + 1.5*width, prosodic_scores, width, label='Prosodia', alpha=0.8)
        
        ax1.set_xlabel('Muestras')
        ax1.set_ylabel('Puntuaci√≥n (0-10)')
        ax1.set_title('Puntuaciones por Categor√≠a')
        ax1.set_xticks(x)
        ax1.set_xticklabels(sample_names, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Gr√°fico 2: Puntuaci√≥n general
        ax2 = axes[0, 1]
        overall_scores = [result["analysis"].scores.overall for result in results]
        bars = ax2.bar(sample_names, overall_scores, color='skyblue', alpha=0.7)
        ax2.set_xlabel('Muestras')
        ax2.set_ylabel('Puntuaci√≥n General (0-10)')
        ax2.set_title('Puntuaci√≥n General por Muestra')
        ax2.grid(True, alpha=0.3)
        
        # A√±adir valores en las barras
        for bar, score in zip(bars, overall_scores):
            height = bar.get_height()
            ax2.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                    f'{score:.1f}', ha='center', va='bottom')
        
        # Gr√°fico 3: Mapa de calor de todas las puntuaciones
        ax3 = axes[1, 0]
        score_matrix = np.array([
            accuracy_scores,
            fluency_scores,
            completeness_scores,
            prosodic_scores
        ])
        
        im = ax3.imshow(score_matrix, cmap='RdYlGn', aspect='auto', vmin=0, vmax=10)
        ax3.set_xticks(range(len(sample_names)))
        ax3.set_xticklabels(sample_names, rotation=45)
        ax3.set_yticks(range(4))
        ax3.set_yticklabels(['Precisi√≥n', 'Fluidez', 'Completitud', 'Prosodia'])
        ax3.set_title('Mapa de Calor - Todas las Puntuaciones')
        
        # A√±adir valores al mapa de calor
        for i in range(4):
            for j in range(len(sample_names)):
                text = ax3.text(j, i, f'{score_matrix[i, j]:.1f}',
                               ha="center", va="center", color="black", fontweight="bold")
        
        # A√±adir barra de colores
        plt.colorbar(im, ax=ax3, label='Puntuaci√≥n')
        
        # Gr√°fico 4: N√∫mero de errores por muestra
        ax4 = axes[1, 1]
        word_errors_count = [len(result["analysis"].word_errors) for result in results]
        phoneme_errors_count = [len(result["analysis"].phoneme_errors) for result in results]
        
        x = np.arange(len(sample_names))
        width = 0.35
        
        ax4.bar(x - width/2, word_errors_count, width, label='Errores de Palabra', alpha=0.8)
        ax4.bar(x + width/2, phoneme_errors_count, width, label='Errores de Fonema', alpha=0.8)
        
        ax4.set_xlabel('Muestras')
        ax4.set_ylabel('N√∫mero de Errores')
        ax4.set_title('Errores Detectados por Muestra')
        ax4.set_xticks(x)
        ax4.set_xticklabels(sample_names, rotation=45)
        ax4.legend()
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.show()
    
    def print_detailed_results(self, results: List[Dict]) -> None:
        """Imprimir resultados detallados"""
        
        print("\n" + "="*80)
        print("RESULTADOS DETALLADOS DEL AN√ÅLISIS DE PRONUNCIACI√ìN")
        print("="*80)
        
        for i, result in enumerate(results):
            sample_info = result["sample_info"]
            analysis = result["analysis"]
            
            print(f"\nüéØ MUESTRA {i+1}: {sample_info['description']}")
            print(f"   Texto: '{sample_info['text']}'")
            print("-" * 60)
            
            # Crear reporte
            report = self.analyzer.create_report(analysis)
            print(report)
            
            # Informaci√≥n adicional
            print(f"\nüìà ESTAD√çSTICAS ADICIONALES:")
            print(f"   ‚Ä¢ N√∫mero de palabras: {len(sample_info['text'].split())}")
            print(f"   ‚Ä¢ Errores de palabra detectados: {len(analysis.word_errors)}")
            print(f"   ‚Ä¢ Errores de fonema detectados: {len(analysis.phoneme_errors)}")
            print(f"   ‚Ä¢ Recomendaciones generadas: {len(analysis.recommendations)}")
            
            if i < len(results) - 1:
                print("\n" + "~" * 60)
    
    def save_results_summary(self, results: List[Dict], output_file: str = "demo_results.json"):
        """Guardar resumen de resultados en JSON"""
        
        import datetime
        summary = {
            "demo_info": {
                "model_path": self.model_path,
                "base_model": self.base_model,
                "total_samples": len(results),
                "timestamp": str(datetime.datetime.now())
            },
            "results": []
        }
        
        for i, result in enumerate(results):
            sample_summary = {
                "sample_id": i + 1,
                "text": result["sample_info"]["text"],
                "description": result["sample_info"]["description"],
                "scores": {
                    "overall": result["analysis"].scores.overall,
                    "accuracy": result["analysis"].scores.accuracy,
                    "fluency": result["analysis"].scores.fluency,
                    "completeness": result["analysis"].scores.completeness,
                    "prosodic": result["analysis"].scores.prosodic
                },
                "errors": {
                    "word_errors": len(result["analysis"].word_errors),
                    "phoneme_errors": len(result["analysis"].phoneme_errors)
                },
                "recommendations_count": len(result["analysis"].recommendations),
                "confidence": result["analysis"].confidence
            }
            summary["results"].append(sample_summary)
        
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        logger.info(f"üìÑ Resumen guardado en: {output_file}")
    
    def run_demo(self, save_results: bool = True, show_visualizations: bool = True):
        """Ejecutar demostraci√≥n completa"""
        
        print("\nüé§ DEMOSTRACI√ìN DE AN√ÅLISIS DE PRONUNCIACI√ìN")
        print("=" * 60)
        print("Este demo muestra las capacidades del modelo Whisper fine-tuneado")
        print("para an√°lisis de pronunciaci√≥n en ingl√©s usando el dataset speechocean762.")
        print("=" * 60)
        
        # Analizar muestras
        logger.info("Generando y analizando muestras de audio...")
        results = self.analyze_real_samples()
        
        if not results:
            logger.error("‚ùå No se pudieron generar resultados")
            return
        
        # Mostrar resultados enfocados en detecci√≥n de errores espec√≠ficos
        self.display_error_detection_results(results)
        
        # Crear visualizaciones de comparaci√≥n
        if show_visualizations:
            try:
                logger.info("Creando visualizaciones de comparaci√≥n...")
                self.create_comparison_visualization(results)
            except Exception as e:
                logger.warning(f"No se pudieron crear visualizaciones: {e}")
        
        # Guardar resultados
        if save_results:
            try:
                self.save_results_summary(results)
            except Exception as e:
                logger.warning(f"No se pudo guardar el resumen: {e}")
        
        # Mostrar resumen final con m√©tricas de comparaci√≥n
        self.show_final_summary_with_comparison(results)

    def display_detailed_results_with_comparison(self, results: List[Dict]) -> None:
        """Mostrar resultados detallados con comparaci√≥n entre scores reales y predichos"""
        
        print("\n" + "="*90)
        print("RESULTADOS DETALLADOS CON COMPARACI√ìN REAL vs PREDICHO")
        print("="*90)
        
        for i, result in enumerate(results):
            sample_info = result["sample_info"]
            analysis = result["analysis"]
            real_scores = result.get("real_scores")
            
            print(f"\nüéØ MUESTRA {i+1}: {sample_info['description']}")
            print(f"   Texto: '{sample_info['text']}'")
            print("-" * 70)
            
            # Mostrar comparaci√≥n de scores si hay datos reales
            if real_scores:
                print("üìä COMPARACI√ìN DE PUNTUACIONES (Real vs Predicho):")
                print(f"   ‚Ä¢ General:     {real_scores['total']:.1f}/10  ‚Üí  {analysis.scores.overall:.1f}/10")
                print(f"   ‚Ä¢ Precisi√≥n:   {real_scores['accuracy']:.1f}/10  ‚Üí  {analysis.scores.accuracy:.1f}/10") 
                print(f"   ‚Ä¢ Fluidez:     {real_scores['fluency']:.1f}/10  ‚Üí  {analysis.scores.fluency:.1f}/10")
                print(f"   ‚Ä¢ Completitud: {real_scores['completeness']:.1f}/1.0  ‚Üí  {analysis.scores.completeness:.1f}/1.0")
                print(f"   ‚Ä¢ Prosodia:    {real_scores['prosodic']:.1f}/10  ‚Üí  {analysis.scores.prosodic:.1f}/10")
                
                # Calcular diferencias
                diff_accuracy = abs(real_scores['accuracy'] - analysis.scores.accuracy)
                diff_fluency = abs(real_scores['fluency'] - analysis.scores.fluency)
                diff_total = abs(real_scores['total'] - analysis.scores.overall)
                
                print(f"\nüìà DIFERENCIAS (Error Absoluto):")
                print(f"   ‚Ä¢ Precisi√≥n: {diff_accuracy:.1f}")
                print(f"   ‚Ä¢ Fluidez: {diff_fluency:.1f}")
                print(f"   ‚Ä¢ General: {diff_total:.1f}")
                
            else:
                print("üìä PUNTUACIONES PREDICHAS:")
                print(f"   ‚Ä¢ General: {analysis.scores.overall:.1f}/10")
                print(f"   ‚Ä¢ Precisi√≥n: {analysis.scores.accuracy:.1f}/10")
                print(f"   ‚Ä¢ Fluidez: {analysis.scores.fluency:.1f}/10")
                print(f"   ‚Ä¢ Completitud: {analysis.scores.completeness:.1f}/1.0")
                print(f"   ‚Ä¢ Prosodia: {analysis.scores.prosodic:.1f}/10")
            
            # Mostrar transcript
            print(f"\nüìù TRANSCRIPCI√ìN: {analysis.transcript}")
            
            # Mostrar errores detectados
            if analysis.word_errors:
                print(f"\n‚ö†Ô∏è  ERRORES DE PALABRAS DETECTADOS:")
                for error in analysis.word_errors:
                    print(f"   ‚Ä¢ {error.word}: {error.accuracy_score:.1f}/10 - {error.suggestion}")
            
            if analysis.phoneme_errors:
                print(f"\nüîä ERRORES DE FONEMAS DETECTADOS:")
                for error in analysis.phoneme_errors:
                    print(f"   ‚Ä¢ En '{error.word}': /{error.canonical_phone}/ ‚Üí /{error.pronounced_phone}/")
            
            # Mostrar recomendaciones
            if analysis.recommendations:
                print(f"\nüí° RECOMENDACIONES:")
                for rec in analysis.recommendations:
                    print(f"   {rec}")
            
            print(f"\nüéØ Confianza del an√°lisis: {analysis.confidence:.1%}")
            print("~" * 70)
    
    def create_comparison_visualization(self, results: List[Dict]) -> None:
        """Crear visualizaciones comparando scores reales vs predichos"""
        
        # Filtrar solo resultados con scores reales
        real_results = [r for r in results if r.get("real_scores")]
        
        if not real_results:
            logger.info("No hay scores reales para comparar, usando visualizaci√≥n est√°ndar")
            return self.create_visualization(results)
        
        # Configurar visualizaci√≥n
        plt.style.use('seaborn-v0_8')
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Comparaci√≥n: Scores Reales vs Predichos del Modelo', fontsize=16, fontweight='bold')
        
        # Extraer datos para comparaci√≥n
        sample_names = [f"Muestra {i+1}" for i in range(len(real_results))]
        
        real_accuracy = [r["real_scores"]["accuracy"] for r in real_results]
        pred_accuracy = [r["analysis"].scores.accuracy for r in real_results]
        
        real_fluency = [r["real_scores"]["fluency"] for r in real_results]
        pred_fluency = [r["analysis"].scores.fluency for r in real_results]
        
        real_total = [r["real_scores"]["total"] for r in real_results]
        pred_total = [r["analysis"].scores.overall for r in real_results]
        
        # Gr√°fico 1: Comparaci√≥n de Precisi√≥n
        ax1 = axes[0, 0]
        x = np.arange(len(sample_names))
        width = 0.35
        
        ax1.bar(x - width/2, real_accuracy, width, label='Real', alpha=0.8, color='skyblue')
        ax1.bar(x + width/2, pred_accuracy, width, label='Predicho', alpha=0.8, color='orange')
        
        ax1.set_xlabel('Muestras')
        ax1.set_ylabel('Precisi√≥n')
        ax1.set_title('Precisi√≥n: Real vs Predicho')
        ax1.set_xticks(x)
        ax1.set_xticklabels(sample_names, rotation=45)
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        # Gr√°fico 2: Comparaci√≥n de Fluidez
        ax2 = axes[0, 1]
        ax2.bar(x - width/2, real_fluency, width, label='Real', alpha=0.8, color='lightgreen')
        ax2.bar(x + width/2, pred_fluency, width, label='Predicho', alpha=0.8, color='coral')
        
        ax2.set_xlabel('Muestras')
        ax2.set_ylabel('Fluidez')
        ax2.set_title('Fluidez: Real vs Predicho')
        ax2.set_xticks(x)
        ax2.set_xticklabels(sample_names, rotation=45)
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        
        # Gr√°fico 3: Scatter plot Real vs Predicho
        ax3 = axes[1, 0]
        ax3.scatter(real_total, pred_total, alpha=0.7, s=100)
        
        # L√≠nea diagonal perfecta
        min_val = min(min(real_total), min(pred_total))
        max_val = max(max(real_total), max(pred_total))
        ax3.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.8, label='Predicci√≥n Perfecta')
        
        ax3.set_xlabel('Score Real')
        ax3.set_ylabel('Score Predicho')
        ax3.set_title('Correlaci√≥n: Real vs Predicho')
        ax3.legend()
        ax3.grid(True, alpha=0.3)
        
        # Gr√°fico 4: Errores absolutos
        ax4 = axes[1, 1]
        errors = [abs(r - p) for r, p in zip(real_total, pred_total)]
        
        bars = ax4.bar(sample_names, errors, alpha=0.8, color='red')
        ax4.set_xlabel('Muestras')
        ax4.set_ylabel('Error Absoluto')
        ax4.set_title('Error Absoluto por Muestra')
        ax4.set_xticklabels(sample_names, rotation=45)
        ax4.grid(True, alpha=0.3)
        
        # A√±adir valores encima de las barras
        for bar, error in zip(bars, errors):
            height = bar.get_height()
            ax4.text(bar.get_x() + bar.get_width()/2., height,
                    f'{error:.1f}', ha='center', va='bottom')
        
        plt.tight_layout()
        plt.show()
    
    def show_final_summary_with_comparison(self, results: List[Dict]) -> None:
        """Mostrar resumen final con m√©tricas de comparaci√≥n"""
        
        real_results = [r for r in results if r.get("real_scores")]
        
        print("\n" + "="*80)
        print("üìä RESUMEN FINAL DEL AN√ÅLISIS")
        print("="*80)
        
        if real_results:
            # Calcular m√©tricas de error
            total_errors = []
            accuracy_errors = []
            fluency_errors = []
            
            for result in real_results:
                real = result["real_scores"]
                pred = result["analysis"].scores
                
                total_errors.append(abs(real["total"] - pred.overall))
                accuracy_errors.append(abs(real["accuracy"] - pred.accuracy))
                fluency_errors.append(abs(real["fluency"] - pred.fluency))
            
            print(f"üéØ **M√©tricas de Evaluaci√≥n del Modelo:**")
            print(f"   ‚Ä¢ Muestras con datos reales: {len(real_results)}")
            print(f"   ‚Ä¢ Error Absoluto Medio (Total): {np.mean(total_errors):.2f}")
            print(f"   ‚Ä¢ Error Absoluto Medio (Precisi√≥n): {np.mean(accuracy_errors):.2f}")
            print(f"   ‚Ä¢ Error Absoluto Medio (Fluidez): {np.mean(fluency_errors):.2f}")
            print(f"   ‚Ä¢ Error M√°ximo: {np.max(total_errors):.2f}")
            print(f"   ‚Ä¢ Error M√≠nimo: {np.min(total_errors):.2f}")
        
        print(f"\nüìà **Resumen General:**")
        print(f"   ‚Ä¢ Total de muestras analizadas: {len(results)}")
        
        avg_confidence = np.mean([r["analysis"].confidence for r in results])
        print(f"   ‚Ä¢ Confianza promedio: {avg_confidence:.1%}")
        
        total_word_errors = sum(len(r["analysis"].word_errors) for r in results)
        total_phoneme_errors = sum(len(r["analysis"].phoneme_errors) for r in results)
        print(f"   ‚Ä¢ Total errores de palabras detectados: {total_word_errors}")
        print(f"   ‚Ä¢ Total errores de fonemas detectados: {total_phoneme_errors}")
        
        if real_results:
            avg_real_score = np.mean([r["real_scores"]["total"] for r in real_results])
            avg_pred_score = np.mean([r["analysis"].scores.overall for r in real_results])
            print(f"   ‚Ä¢ Score real promedio: {avg_real_score:.2f}/10")
            print(f"   ‚Ä¢ Score predicho promedio: {avg_pred_score:.2f}/10")
        
        print("\n‚úÖ **An√°lisis completado con √©xito!**")
        
        if real_results:
            print("üéØ **El modelo fue evaluado con datos reales del dataset speechocean762**")
            mae = np.mean(total_errors)
            if mae < 1.0:
                print("üü¢ **Rendimiento: EXCELENTE** (Error < 1.0)")
            elif mae < 2.0:
                print("üü° **Rendimiento: BUENO** (Error < 2.0)")
            else:
                print("üî¥ **Rendimiento: NECESITA MEJORA** (Error > 2.0)")
        
        print("="*80)

    def extract_real_pronunciation_errors(self, sample_data: Dict) -> Dict:
        """Extraer errores espec√≠ficos de pronunciaci√≥n del dataset real"""
        words_data = sample_data.get("words", [])
        
        real_errors = {
            "word_errors": [],
            "phoneme_errors": [],
            "total_error_words": 0,
            "total_mispronunciations": 0
        }
        
        for word_info in words_data:
            word_text = word_info.get("text", "")
            word_accuracy = word_info.get("accuracy", 10.0)
            mispronunciations = word_info.get("mispronunciations", [])
            
            # Palabras con errores (accuracy < 8)
            if word_accuracy < 8.0:
                real_errors["word_errors"].append({
                    "word": word_text,
                    "accuracy": word_accuracy,
                    "total": word_info.get("total", 10.0)
                })
                real_errors["total_error_words"] += 1
            
            # Errores espec√≠ficos de fonemas  
            for mispron in mispronunciations:
                real_errors["phoneme_errors"].append({
                    "word": word_text,
                    "canonical": mispron.get("canonical-phone", ""),
                    "pronounced": mispron.get("pronounced-phone", ""),
                    "index": mispron.get("index", 0)
                })
                real_errors["total_mispronunciations"] += 1
        
        return real_errors

    def compare_error_detection(self, predicted_analysis, real_errors: Dict, sample_text: str) -> Dict:
        """Comparar errores detectados por el modelo vs errores reales del dataset"""
        
        # Extraer palabras con errores predichos
        predicted_error_words = set()
        if predicted_analysis.word_errors:
            predicted_error_words = {error.word.lower() for error in predicted_analysis.word_errors}
        
        # Extraer palabras con errores reales
        real_error_words = set()
        if real_errors["word_errors"]:
            real_error_words = {error["word"].lower() for error in real_errors["word_errors"]}
        
        # Calcular m√©tricas de detecci√≥n
        true_positives = len(predicted_error_words.intersection(real_error_words))
        false_positives = len(predicted_error_words - real_error_words)
        false_negatives = len(real_error_words - predicted_error_words)
        
        precision = true_positives / len(predicted_error_words) if predicted_error_words else 0
        recall = true_positives / len(real_error_words) if real_error_words else 0
        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
        
        return {
            "predicted_errors": list(predicted_error_words),
            "real_errors": list(real_error_words),
            "correctly_detected": list(predicted_error_words.intersection(real_error_words)),
            "missed_errors": list(real_error_words - predicted_error_words),
            "false_alarms": list(predicted_error_words - real_error_words),
            "metrics": {
                "precision": precision,
                "recall": recall,
                "f1_score": f1_score,
                "true_positives": true_positives,
                "false_positives": false_positives,
                "false_negatives": false_negatives
            }
        }

    def display_error_detection_results(self, results: List[Dict]) -> None:
        """Mostrar resultados enfocados en detecci√≥n de errores espec√≠ficos"""
        
        print("\n" + "="*90)
        print("üîç AN√ÅLISIS DE DETECCI√ìN DE ERRORES DE PRONUNCIACI√ìN")
        print("="*90)
        print("Comparaci√≥n: Errores Reales del Dataset vs Errores Detectados por el Modelo")
        print("="*90)
        
        total_metrics = {
            "total_real_errors": 0,
            "total_predicted_errors": 0,
            "total_correctly_detected": 0,
            "total_missed": 0,
            "total_false_alarms": 0
        }
        
        for i, result in enumerate(results):
            sample_info = result["sample_info"]
            
            # Extraer errores reales del dataset
            real_errors = self.extract_real_pronunciation_errors(sample_info)
            
            # Comparar con errores detectados por el modelo
            error_comparison = self.compare_error_detection(
                result["analysis"], 
                real_errors, 
                sample_info["text"]
            )
            
            print(f"\nüéØ MUESTRA {i+1}: {sample_info['description']}")
            print(f"   Texto: '{sample_info['text']}'")
            print("-" * 70)
            
            # Mostrar respuesta REAL del modelo
            print(f"ü§ñ RESPUESTA RAW DEL MODELO:")
            print(f"   '{result['analysis'].generated_analysis[:200]}...'")
            
            # An√°lisis de errores de palabras
            print(f"\nüìä ERRORES DE PALABRAS:")
            print(f"   üéØ Errores reales en dataset: {len(real_errors['word_errors'])} palabras")
            if real_errors["word_errors"]:
                for error in real_errors["word_errors"]:
                    print(f"      ‚Ä¢ {error['word']}: {error['accuracy']:.1f}/10 (real)")
            else:
                print("      ‚Ä¢ No hay errores de palabras en el dataset")
            
            print(f"   üîç Errores detectados por modelo: {len(error_comparison['predicted_errors'])} palabras")
            if error_comparison["predicted_errors"]:
                for word in error_comparison["predicted_errors"]:
                    print(f"      ‚Ä¢ {word} (detectado)")
            else:
                print("      ‚Ä¢ Modelo no detect√≥ errores de palabras")
            
            # M√©tricas de detecci√≥n
            metrics = error_comparison["metrics"]
            print(f"\nüìà M√âTRICAS DE DETECCI√ìN:")
            print(f"   ‚úÖ Correctamente detectados: {metrics['true_positives']} palabras")
            print(f"   ‚ùå Errores perdidos: {metrics['false_negatives']} palabras")
            print(f"   üö® Falsas alarmas: {metrics['false_positives']} palabras")
            print(f"   üéØ Precisi√≥n: {metrics['precision']:.2f}")
            print(f"   üìç Recall: {metrics['recall']:.2f}")
            print(f"   üèÜ F1-Score: {metrics['f1_score']:.2f}")
            
            if error_comparison["correctly_detected"]:
                print(f"   ‚úÖ Palabras bien detectadas: {', '.join(error_comparison['correctly_detected'])}")
            if error_comparison["missed_errors"]:
                print(f"   ‚ùå Errores no detectados: {', '.join(error_comparison['missed_errors'])}")
            if error_comparison["false_alarms"]:
                print(f"   üö® Falsas alarmas: {', '.join(error_comparison['false_alarms'])}")
            
            # An√°lisis de fonemas
            print(f"\nüî§ ERRORES DE FONEMAS:")
            print(f"   Dataset real: {len(real_errors['phoneme_errors'])} errores de fonemas")
            if real_errors["phoneme_errors"]:
                for error in real_errors["phoneme_errors"][:3]:  # Mostrar solo los primeros 3
                    print(f"      ‚Ä¢ En '{error['word']}': '{error['canonical']}' ‚Üí '{error['pronounced']}'")
                if len(real_errors["phoneme_errors"]) > 3:
                    print(f"      ‚Ä¢ ... y {len(real_errors['phoneme_errors']) - 3} errores m√°s")
            
            # Actualizar m√©tricas totales
            total_metrics["total_real_errors"] += len(real_errors["word_errors"])
            total_metrics["total_predicted_errors"] += len(error_comparison["predicted_errors"])
            total_metrics["total_correctly_detected"] += metrics["true_positives"]
            total_metrics["total_missed"] += metrics["false_negatives"]
            total_metrics["total_false_alarms"] += metrics["false_positives"]
            
            print("~" * 70)
        
        # Resumen final de detecci√≥n de errores
        print(f"\n" + "="*90)
        print("üìä RESUMEN FINAL DE DETECCI√ìN DE ERRORES")
        print("="*90)
        
        overall_precision = (total_metrics["total_correctly_detected"] / 
                           total_metrics["total_predicted_errors"] 
                           if total_metrics["total_predicted_errors"] > 0 else 0)
        
        overall_recall = (total_metrics["total_correctly_detected"] / 
                         total_metrics["total_real_errors"] 
                         if total_metrics["total_real_errors"] > 0 else 0)
        
        overall_f1 = (2 * overall_precision * overall_recall / 
                     (overall_precision + overall_recall) 
                     if (overall_precision + overall_recall) > 0 else 0)
        
        print(f"üéØ **Rendimiento del Modelo en Detecci√≥n de Errores:**")
        print(f"   ‚Ä¢ Total errores reales en dataset: {total_metrics['total_real_errors']}")
        print(f"   ‚Ä¢ Total errores detectados por modelo: {total_metrics['total_predicted_errors']}")
        print(f"   ‚Ä¢ Correctamente detectados: {total_metrics['total_correctly_detected']}")
        print(f"   ‚Ä¢ Errores perdidos: {total_metrics['total_missed']}")
        print(f"   ‚Ä¢ Falsas alarmas: {total_metrics['total_false_alarms']}")
        print(f"")
        print(f"üìà **M√©tricas Generales:**")
        print(f"   ‚Ä¢ Precisi√≥n: {overall_precision:.3f}")
        print(f"   ‚Ä¢ Recall: {overall_recall:.3f}")
        print(f"   ‚Ä¢ F1-Score: {overall_f1:.3f}")
        
        # Evaluaci√≥n del rendimiento
        if overall_f1 >= 0.7:
            performance = "üü¢ EXCELENTE"
        elif overall_f1 >= 0.5:
            performance = "üü° BUENO"
        elif overall_f1 >= 0.3:
            performance = "üü† NECESITA MEJORA"
        else:
            performance = "üî¥ POBRE"
        
        print(f"   ‚Ä¢ Rendimiento General: {performance}")
        print("="*90)


def main():
    """Funci√≥n principal"""
    parser = argparse.ArgumentParser(
        description="Demostraci√≥n del analizador de pronunciaci√≥n",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Ejemplos de uso:
  # Ejecutar demo completo
  python demo_inference.py

  # Usar modelo personalizado
  python demo_inference.py --model-path ./mi_modelo --base-model openai/whisper-base

  # Solo mostrar resultados, sin visualizaciones
  python demo_inference.py --no-visualizations
        """
    )
    
    parser.add_argument(
        "--model-path",
        default="./whisper-pronunciation-tuned",
        help="Ruta del modelo fine-tuneado"
    )
    
    parser.add_argument(
        "--base-model",
        default="openai/whisper-small",
        help="Modelo base de Whisper usado"
    )
    
    parser.add_argument(
        "--no-visualizations",
        action="store_true",
        help="Desactivar visualizaciones"
    )
    
    parser.add_argument(
        "--no-save",
        action="store_true",
        help="No guardar resultados"
    )
    
    args = parser.parse_args()
    
    try:
        # Crear y ejecutar demo
        demo = PronunciationDemo(
            model_path=args.model_path,
            base_model=args.base_model
        )
        
        demo.run_demo(
            save_results=not args.no_save,
            show_visualizations=not args.no_visualizations
        )
        
    except FileNotFoundError as e:
        logger.error(f"‚ùå {e}")
        logger.info("üí° Aseg√∫rate de haber entrenado el modelo primero:")
        logger.info("   python main.py train")
        sys.exit(1)
        
    except Exception as e:
        logger.error(f"‚ùå Error ejecutando demo: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 